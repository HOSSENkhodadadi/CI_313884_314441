{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Running the Code**\n",
        "Following note, includes the training process of the Deep RL agent.\n",
        "1.   first execute the next note (Game class)"
      ],
      "metadata": {
        "id": "TosExp1h98pk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vcthCM26p0Mf"
      },
      "outputs": [],
      "source": [
        "from abc import ABC, abstractmethod\n",
        "from copy import deepcopy\n",
        "from enum import Enum\n",
        "import numpy as np\n",
        "\n",
        "# Rules on PDF and https://cdn.1j1ju.com/medias/a8/5e/26-quixo-rulebook.pdf\n",
        "\n",
        "\n",
        "class Move(Enum):\n",
        "    '''\n",
        "    Selects where you want to place the taken piece. The rest of the pieces are shifted\n",
        "    '''\n",
        "    TOP = 0\n",
        "    BOTTOM = 1\n",
        "    LEFT = 2\n",
        "    RIGHT = 3\n",
        "\n",
        "\n",
        "class Player(ABC):\n",
        "    def __init__(self) -> None:\n",
        "        '''You can change this for your player if you need to handle state/have memory'''\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def make_move(self, game: 'Game') -> tuple[tuple[int, int], Move]:\n",
        "        '''\n",
        "        The game accepts coordinates of the type (X, Y). X goes from left to right, while Y goes from top to bottom, as in 2D graphics.\n",
        "        Thus, the coordinates that this method returns shall be in the (X, Y) format.\n",
        "\n",
        "        game: the Quixo game. You can use it to override the current game with yours, but everything is evaluated by the main game\n",
        "        return values: this method shall return a tuple of X,Y positions and a move among TOP, BOTTOM, LEFT and RIGHT\n",
        "        '''\n",
        "        pass\n",
        "\n",
        "\n",
        "class Game(object):\n",
        "    def __init__(self) -> None:\n",
        "        self._board = np.ones((5, 5), dtype=np.uint8) * -1\n",
        "        self.current_player_idx = 1\n",
        "    # def get_current_player(self):\n",
        "    #     return self.current_player_idx\n",
        "\n",
        "    def get_board(self) -> np.ndarray:\n",
        "        '''\n",
        "        Returns the board\n",
        "        '''\n",
        "        return deepcopy(self._board)\n",
        "\n",
        "    def get_current_player(self) -> int:\n",
        "        '''\n",
        "        Returns the current player\n",
        "        '''\n",
        "        return deepcopy(self.current_player_idx)\n",
        "\n",
        "    def print(self):\n",
        "        '''Prints the board. -1 are neutral pieces, 0 are pieces of player 0, 1 pieces of player 1'''\n",
        "        print(self._board)\n",
        "\n",
        "    def check_winner(self) -> int:\n",
        "        '''Check the winner. Returns the player ID of the winner if any, otherwise returns -1'''\n",
        "        # for each row\n",
        "        player = self.get_current_player()\n",
        "        winner = -1\n",
        "        for x in range(self._board.shape[0]):\n",
        "            # if a player has completed an entire row\n",
        "            if self._board[x, 0] != -1 and all(self._board[x, :] == self._board[x, 0]):\n",
        "                # return winner is this guy\n",
        "                winner = self._board[x, 0]\n",
        "        if winner > -1 and winner != self.get_current_player():\n",
        "            return winner\n",
        "        # for each column\n",
        "        for y in range(self._board.shape[1]):\n",
        "            # if a player has completed an entire column\n",
        "            if self._board[0, y] != -1 and all(self._board[:, y] == self._board[0, y]):\n",
        "                # return the relative id\n",
        "                winner = self._board[0, y]\n",
        "        if winner > -1 and winner != self.get_current_player():\n",
        "            return winner\n",
        "        # if a player has completed the principal diagonal\n",
        "        if self._board[0, 0] != -1 and all(\n",
        "            [self._board[x, x]\n",
        "                for x in range(self._board.shape[0])] == self._board[0, 0]\n",
        "        ):\n",
        "            # return the relative id\n",
        "            winner = self._board[0, 0]\n",
        "        if winner > -1 and winner != self.get_current_player():\n",
        "            return winner\n",
        "        # if a player has completed the secondary diagonal\n",
        "        if self._board[0, -1] != -1 and all(\n",
        "            [self._board[x, -(x + 1)]\n",
        "             for x in range(self._board.shape[0])] == self._board[0, -1]\n",
        "        ):\n",
        "            # return the relative id\n",
        "            winner = self._board[0, -1]\n",
        "        return winner\n",
        "\n",
        "    def play(self, player1: Player, player2: Player) -> int:\n",
        "        '''Play the game. Returns the winning player'''\n",
        "        players = [player1, player2]\n",
        "        winner = -1\n",
        "        while winner < 0:\n",
        "            self.current_player_idx += 1\n",
        "            self.current_player_idx %= len(players)\n",
        "            ok = False\n",
        "            while not ok:\n",
        "                from_pos, slide = players[self.current_player_idx].make_move(\n",
        "                    self)\n",
        "                ok = self.__move(from_pos, slide, self.current_player_idx)\n",
        "            winner = self.check_winner()\n",
        "        return winner\n",
        "\n",
        "    def __move(self, from_pos: tuple[int, int], slide: Move, player_id: int) -> bool:\n",
        "        '''Perform a move'''\n",
        "        if player_id > 2:\n",
        "            return False\n",
        "        # Oh God, Numpy arrays\n",
        "        prev_value = deepcopy(self._board[(from_pos[1], from_pos[0])])\n",
        "        acceptable = self.__take((from_pos[1], from_pos[0]), player_id)\n",
        "        if acceptable:\n",
        "            acceptable = self.__slide((from_pos[1], from_pos[0]), slide)\n",
        "            if not acceptable:\n",
        "                self._board[(from_pos[1], from_pos[0])] = deepcopy(prev_value)\n",
        "        return acceptable\n",
        "\n",
        "    def __take(self, from_pos: tuple[int, int], player_id: int) -> bool:\n",
        "        '''Take piece'''\n",
        "        # acceptable only if in border\n",
        "        acceptable: bool = (\n",
        "            # check if it is in the first row\n",
        "            (from_pos[0] == 0 and from_pos[1] < 5)\n",
        "            # check if it is in the last row\n",
        "            or (from_pos[0] == 4 and from_pos[1] < 5)\n",
        "            # check if it is in the first column\n",
        "            or (from_pos[1] == 0 and from_pos[0] < 5)\n",
        "            # check if it is in the last column\n",
        "            or (from_pos[1] == 4 and from_pos[0] < 5)\n",
        "            # and check if the piece can be moved by the current player\n",
        "        ) and (self._board[from_pos] < 0 or self._board[from_pos] == player_id)\n",
        "        if acceptable:\n",
        "            self._board[from_pos] = player_id\n",
        "        return acceptable\n",
        "\n",
        "    def __slide(self, from_pos: tuple[int, int], slide: Move) -> bool:\n",
        "        '''Slide the other pieces'''\n",
        "        # define the corners\n",
        "        SIDES = [(0, 0), (0, 4), (4, 0), (4, 4)]\n",
        "        # if the piece position is not in a corner\n",
        "        if from_pos not in SIDES:\n",
        "            # if it is at the TOP, it can be moved down, left or right\n",
        "            acceptable_top: bool = from_pos[0] == 0 and (\n",
        "                slide == Move.BOTTOM or slide == Move.LEFT or slide == Move.RIGHT\n",
        "            )\n",
        "            # if it is at the BOTTOM, it can be moved up, left or right\n",
        "            acceptable_bottom: bool = from_pos[0] == 4 and (\n",
        "                slide == Move.TOP or slide == Move.LEFT or slide == Move.RIGHT\n",
        "            )\n",
        "            # if it is on the LEFT, it can be moved up, down or right\n",
        "            acceptable_left: bool = from_pos[1] == 0 and (\n",
        "                slide == Move.BOTTOM or slide == Move.TOP or slide == Move.RIGHT\n",
        "            )\n",
        "            # if it is on the RIGHT, it can be moved up, down or left\n",
        "            acceptable_right: bool = from_pos[1] == 4 and (\n",
        "                slide == Move.BOTTOM or slide == Move.TOP or slide == Move.LEFT\n",
        "            )\n",
        "        # if the piece position is in a corner\n",
        "        else:\n",
        "            # if it is in the upper left corner, it can be moved to the right and down\n",
        "            acceptable_top: bool = from_pos == (0, 0) and (\n",
        "                slide == Move.BOTTOM or slide == Move.RIGHT)\n",
        "            # if it is in the lower left corner, it can be moved to the right and up\n",
        "            acceptable_left: bool = from_pos == (4, 0) and (\n",
        "                slide == Move.TOP or slide == Move.RIGHT)\n",
        "            # if it is in the upper right corner, it can be moved to the left and down\n",
        "            acceptable_right: bool = from_pos == (0, 4) and (\n",
        "                slide == Move.BOTTOM or slide == Move.LEFT)\n",
        "            # if it is in the lower right corner, it can be moved to the left and up\n",
        "            acceptable_bottom: bool = from_pos == (4, 4) and (\n",
        "                slide == Move.TOP or slide == Move.LEFT)\n",
        "        # check if the move is acceptable\n",
        "        acceptable: bool = acceptable_top or acceptable_bottom or acceptable_left or acceptable_right\n",
        "        # if it is\n",
        "        if acceptable:\n",
        "            # take the piece\n",
        "            piece = self._board[from_pos]\n",
        "            # if the player wants to slide it to the left\n",
        "            if slide == Move.LEFT:\n",
        "                # for each column starting from the column of the piece and moving to the left\n",
        "                for i in range(from_pos[1], 0, -1):\n",
        "                    # copy the value contained in the same row and the previous column\n",
        "                    self._board[(from_pos[0], i)] = self._board[(\n",
        "                        from_pos[0], i - 1)]\n",
        "                # move the piece to the left\n",
        "                self._board[(from_pos[0], 0)] = piece\n",
        "            # if the player wants to slide it to the right\n",
        "            elif slide == Move.RIGHT:\n",
        "                # for each column starting from the column of the piece and moving to the right\n",
        "                for i in range(from_pos[1], self._board.shape[1] - 1, 1):\n",
        "                    # copy the value contained in the same row and the following column\n",
        "                    self._board[(from_pos[0], i)] = self._board[(\n",
        "                        from_pos[0], i + 1)]\n",
        "                # move the piece to the right\n",
        "                self._board[(from_pos[0], self._board.shape[1] - 1)] = piece\n",
        "            # if the player wants to slide it upward\n",
        "            elif slide == Move.TOP:\n",
        "                # for each row starting from the row of the piece and going upward\n",
        "                for i in range(from_pos[0], 0, -1):\n",
        "                    # copy the value contained in the same column and the previous row\n",
        "                    self._board[(i, from_pos[1])] = self._board[(\n",
        "                        i - 1, from_pos[1])]\n",
        "                # move the piece up\n",
        "                self._board[(0, from_pos[1])] = piece\n",
        "            # if the player wants to slide it downward\n",
        "            elif slide == Move.BOTTOM:\n",
        "                # for each row starting from the row of the piece and going downward\n",
        "                for i in range(from_pos[0], self._board.shape[0] - 1, 1):\n",
        "                    # copy the value contained in the same column and the following row\n",
        "                    self._board[(i, from_pos[1])] = self._board[(\n",
        "                        i + 1, from_pos[1])]\n",
        "                # move the piece down\n",
        "                self._board[(self._board.shape[0] - 1, from_pos[1])] = piece\n",
        "        return acceptable"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.   following note generates the possible actions, given the current game board (excluding already occupied elements)"
      ],
      "metadata": {
        "id": "43MbKKRADzFD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_possible_moves(game: 'Game') -> list[tuple[tuple[int, int], Move]]:\n",
        "\n",
        "    # possible moves:\n",
        "    # - take border empty and fill the hole by moving in the 3 directions\n",
        "    # - take one of your blocks on the border and fill the hole by moving in the 3 directions\n",
        "    # 44 at start possible moves\n",
        "    pos = set()\n",
        "    for r in [0, 4]:\n",
        "        for c in range(5):\n",
        "            if game.get_board()[r, c] == -1:\n",
        "                if r == 0 and c == 0:  # OK\n",
        "                    pos.add(((c, r), Move.BOTTOM))\n",
        "                    pos.add(((c, r), Move.RIGHT))\n",
        "                elif r == 0 and c == 4:  # OK\n",
        "                    pos.add(((c, r), Move.BOTTOM))\n",
        "                    pos.add(((c, r), Move.LEFT))\n",
        "                elif r == 4 and c == 0:  # OK\n",
        "                    pos.add(((c, r), Move.TOP))\n",
        "                    pos.add(((c, r), Move.RIGHT))\n",
        "                elif r == 4 and c == 4:  # OK\n",
        "                    pos.add(((c, r), Move.TOP))\n",
        "                    pos.add(((c, r), Move.LEFT))\n",
        "                elif r == 0:  # OK\n",
        "                    pos.add(((c, r), Move.BOTTOM))\n",
        "                    pos.add(((c, r), Move.LEFT))\n",
        "                    pos.add(((c, r), Move.RIGHT))\n",
        "                elif r == 4:  # OK\n",
        "                    pos.add(((c, r), Move.TOP))\n",
        "                    pos.add(((c, r), Move.LEFT))\n",
        "                    pos.add(((c, r), Move.RIGHT))\n",
        "    for c in [0, 4]:\n",
        "        for r in range(5):\n",
        "            if game.get_board()[r, c] == -1 or game.get_board()[r, c] == player:\n",
        "                if r == 0 and c == 0:  # OK\n",
        "                    pos.add(((c, r), Move.BOTTOM))\n",
        "                    pos.add(((c, r), Move.RIGHT))\n",
        "                elif r == 0 and c == 4:  # OK\n",
        "                    pos.add(((c, r), Move.BOTTOM))\n",
        "                    pos.add(((c, r), Move.LEFT))\n",
        "                elif r == 4 and c == 0:  # OK\n",
        "                    pos.add(((c, r), Move.TOP))\n",
        "                    pos.add(((c, r), Move.RIGHT))\n",
        "                elif r == 4 and c == 4:  # OK\n",
        "                    pos.add(((c, r), Move.TOP))\n",
        "                    pos.add(((c, r), Move.LEFT))\n",
        "                elif c == 0:\n",
        "                    pos.add(((c, r), Move.TOP))\n",
        "                    pos.add(((c, r), Move.RIGHT))\n",
        "                    pos.add(((c, r), Move.BOTTOM))\n",
        "                elif c == 4:\n",
        "                    pos.add(((c, r), Move.TOP))\n",
        "                    pos.add(((c, r), Move.LEFT))\n",
        "                    pos.add(((c, r), Move.BOTTOM))\n",
        "    return list(pos)\n",
        "\n",
        "g=Game()\n",
        "print(get_possible_moves(g))\n",
        "print(len(get_possible_moves(g)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mM0rxehQp6nz",
        "outputId": "288a2e10-aced-450a-b920-2de96aa84a07"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[((2, 0), <Move.LEFT: 2>), ((0, 3), <Move.TOP: 0>), ((3, 0), <Move.BOTTOM: 1>), ((3, 4), <Move.TOP: 0>), ((4, 1), <Move.BOTTOM: 1>), ((0, 4), <Move.TOP: 0>), ((1, 0), <Move.LEFT: 2>), ((4, 2), <Move.BOTTOM: 1>), ((4, 1), <Move.TOP: 0>), ((4, 3), <Move.BOTTOM: 1>), ((4, 2), <Move.TOP: 0>), ((3, 4), <Move.LEFT: 2>), ((2, 4), <Move.RIGHT: 3>), ((1, 4), <Move.RIGHT: 3>), ((4, 0), <Move.LEFT: 2>), ((0, 0), <Move.RIGHT: 3>), ((4, 3), <Move.TOP: 0>), ((2, 0), <Move.RIGHT: 3>), ((0, 2), <Move.RIGHT: 3>), ((4, 1), <Move.LEFT: 2>), ((3, 0), <Move.LEFT: 2>), ((4, 4), <Move.TOP: 0>), ((0, 0), <Move.BOTTOM: 1>), ((0, 1), <Move.RIGHT: 3>), ((1, 4), <Move.TOP: 0>), ((4, 2), <Move.LEFT: 2>), ((2, 0), <Move.BOTTOM: 1>), ((2, 4), <Move.TOP: 0>), ((1, 0), <Move.RIGHT: 3>), ((0, 2), <Move.BOTTOM: 1>), ((0, 1), <Move.BOTTOM: 1>), ((4, 3), <Move.LEFT: 2>), ((0, 2), <Move.TOP: 0>), ((1, 0), <Move.BOTTOM: 1>), ((0, 1), <Move.TOP: 0>), ((0, 3), <Move.RIGHT: 3>), ((3, 4), <Move.RIGHT: 3>), ((4, 4), <Move.LEFT: 2>), ((1, 4), <Move.LEFT: 2>), ((0, 4), <Move.RIGHT: 3>), ((2, 4), <Move.LEFT: 2>), ((0, 3), <Move.BOTTOM: 1>), ((4, 0), <Move.BOTTOM: 1>), ((3, 0), <Move.RIGHT: 3>)]\n",
            "44\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.   Here we generate any possible position in the game along with any possible action, these are infact the output classes of our deep neural network."
      ],
      "metadata": {
        "id": "QPibdLgVEFSi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classes = [\n",
        "    ((0, 3), Move.RIGHT),\n",
        "    ((3, 0), Move.BOTTOM),\n",
        "    ((3, 4), Move.RIGHT),\n",
        "    ((4, 2), Move.BOTTOM),\n",
        "    ((0, 4), Move.RIGHT),\n",
        "    ((3, 4), Move.LEFT),\n",
        "    ((0, 3), Move.TOP),\n",
        "    ((4, 0), Move.LEFT),\n",
        "    ((3, 0), Move.RIGHT),\n",
        "    ((3, 4), Move.TOP),\n",
        "    ((0, 4), Move.TOP),\n",
        "    ((4, 1), Move.LEFT),\n",
        "    ((4, 3), Move.BOTTOM),\n",
        "    ((4, 1), Move.TOP),\n",
        "    ((3, 0), Move.LEFT),\n",
        "    ((4, 2), Move.LEFT),\n",
        "    ((4, 2), Move.TOP),\n",
        "    ((4, 3), Move.LEFT),\n",
        "    ((0, 0), Move.BOTTOM),\n",
        "    ((1, 4), Move.RIGHT),\n",
        "    ((4, 3), Move.TOP),\n",
        "    ((2, 0), Move.BOTTOM),\n",
        "    ((2, 4), Move.RIGHT),\n",
        "    ((0, 2), Move.BOTTOM),\n",
        "    ((0, 1), Move.BOTTOM),\n",
        "    ((0, 0), Move.RIGHT),\n",
        "    ((4, 4), Move.LEFT),\n",
        "    ((4, 4), Move.TOP),\n",
        "    ((1, 0), Move.BOTTOM),\n",
        "    ((2, 4), Move.LEFT),\n",
        "    ((1, 4), Move.LEFT),\n",
        "    ((2, 0), Move.RIGHT),\n",
        "    ((2, 4), Move.TOP),\n",
        "    ((1, 4), Move.TOP),\n",
        "    ((0, 2), Move.RIGHT),\n",
        "    ((0, 1), Move.RIGHT),\n",
        "    ((0, 3), Move.BOTTOM),\n",
        "    ((1, 0), Move.RIGHT),\n",
        "    ((2, 0), Move.LEFT),\n",
        "    ((4, 0), Move.BOTTOM),\n",
        "    ((0, 2), Move.TOP),\n",
        "    ((0, 1), Move.TOP),\n",
        "    ((1, 0), Move.LEFT),\n",
        "    ((4, 1), Move.BOTTOM)\n",
        "]\n"
      ],
      "metadata": {
        "id": "iGf_9XOs0ADT"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.   Following is the definition of our agent, including our model architecture, make_move and train functions."
      ],
      "metadata": {
        "id": "4wdwA5PzEesA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from collections import deque\n",
        "\n",
        "# Defines the neural network architecture\n",
        "class ComplexDQN(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(ComplexDQN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 256)\n",
        "        self.fc2 = nn.Linear(256, 512)\n",
        "        self.fc3 = nn.Linear(512, 512)\n",
        "        self.fc4 = nn.Linear(512, 256)\n",
        "        self.fc5 = nn.Linear(256, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = F.relu(self.fc4(x))\n",
        "        return self.fc5(x)\n",
        "\n",
        "\n",
        "# Defines constants\n",
        "input_size = 5 * 5  # Size of the flattened game board\n",
        "output_size = 44     # Number of possible moves (from_pos, move)\n",
        "hidden_size = 64    # Size of the hidden layer in the neural network\n",
        "learning_rate = 0.001\n",
        "epsilon = 0.01       # Epsilon for epsilon-greedy policy\n",
        "batch_size = 32     # Batch size for training the neural network\n",
        "memory_size = 10000 # Size of the experience replay buffer\n",
        "\n",
        "# Let's define Deep RL player\n",
        "class DeepRLPlayer(Player):\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model = ComplexDQN(input_size, output_size).to(self.device)\n",
        "        self.target_model = ComplexDQN(input_size, output_size).to(self.device)\n",
        "        self.target_model.load_state_dict(self.model.state_dict())\n",
        "        self.target_model.eval()\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
        "        self.memory = deque(maxlen=memory_size)\n",
        "        self.steps_done = 0\n",
        "        self.gamma = 0.99  # Discount factor for future rewards\n",
        "\n",
        "    def make_move(self, game: 'Game') -> tuple[tuple[int, int], Move]:\n",
        "        state = torch.tensor(game.get_board().flatten(), dtype=torch.float32).unsqueeze(0).to(self.device)\n",
        "\n",
        "        # Choose action using epsilon-greedy policy\n",
        "        if random.random() < epsilon:\n",
        "            action = random.choice([Move.TOP, Move.BOTTOM, Move.LEFT, Move.RIGHT])\n",
        "            from_pos = (random.randint(0, 4), random.randint(0, 4))\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                q_values = self.model(state)\n",
        "                from_pos = classes[torch.argmax(q_values).item()][0]\n",
        "                action = classes[torch.argmax(q_values).item()][1]\n",
        "        return from_pos, action\n",
        "\n",
        "    def train(self):\n",
        "        if len(self.memory) < batch_size:\n",
        "            return\n",
        "        minibatch = random.sample(self.memory, batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*minibatch)\n",
        "        states = torch.tensor(states, dtype=torch.float32).to(self.device)\n",
        "        actions = torch.tensor(actions, dtype=torch.int64).to(self.device)\n",
        "        rewards = torch.tensor(rewards, dtype=torch.float32).to(self.device)\n",
        "        next_states = torch.tensor(next_states, dtype=torch.float32).to(self.device)\n",
        "        dones = torch.tensor(dones, dtype=torch.float32).to(self.device)\n",
        "\n",
        "        current_q_values = self.model(states).gather(1, actions.unsqueeze(1))\n",
        "        next_q_values = self.target_model(next_states).max(1)[0].detach()\n",
        "        target_q_values = rewards + (1 - dones) * self.gamma * next_q_values\n",
        "\n",
        "        loss = F.smooth_l1_loss(current_q_values, target_q_values.unsqueeze(1))\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # Update target network\n",
        "        if self.steps_done % target_update == 0:\n",
        "            self.target_model.load_state_dict(self.model.state_dict())\n",
        "\n",
        "        self.steps_done += 1\n",
        "\n",
        "    def store_experience(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "# Define constants\n",
        "target_update = 1000  # Frequency of updating the target network"
      ],
      "metadata": {
        "id": "A_sGfIYZqofv"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This one will be used in testing mode, skip it."
      ],
      "metadata": {
        "id": "LhwJg3hEFL1p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Define a directory to save the trained models\n",
        "MODEL_DIR = 'trained_models'\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)"
      ],
      "metadata": {
        "id": "eVQBtkSO9bXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This one will be used in testing mode as well, skip it."
      ],
      "metadata": {
        "id": "VqFgHGMRFRup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "model_filename = \"deep_rl_player_10.pth, TrainWinRate_82.53,roundNum_ 10000, lr_, 0.001, epsilon_0.01, structure_ 256_512\"\n",
        "\n",
        "# Function to load model parameters\n",
        "def load_model(model, filename):\n",
        "    filepath = os.path.join(MODEL_DIR, filename)\n",
        "    model.load_state_dict(torch.load(filepath))\n",
        "    print(f\"Model loaded from '{filepath}'\")\n",
        "\n",
        "g = Game()\n",
        "player1 = RandomPlayer()\n",
        "player2 = DeepRLPlayer()\n",
        "\n",
        "# Load the trained model\n",
        "load_model(player2.model, model_filename)\n",
        "print(\"Model loaded.\")"
      ],
      "metadata": {
        "id": "sQ5FevWO9EpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.   Run to create the random player."
      ],
      "metadata": {
        "id": "_RGYol7fFZGs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "class RandomPlayer(Player):\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "    def make_move(self, game: 'Game') -> tuple[tuple[int, int], Move]:\n",
        "        from_pos = (random.randint(0, 4), random.randint(0, 4))\n",
        "        move = random.choice([Move.TOP, Move.BOTTOM, Move.LEFT, Move.RIGHT])\n",
        "        return from_pos, move"
      ],
      "metadata": {
        "id": "EfyOWpr3rPsf"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.   Following code includes the training process."
      ],
      "metadata": {
        "id": "UmmXifAyFkBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define a directory to save the trained models\n",
        "MODEL_DIR = 'trained_models'\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "# Function to save model parameters\n",
        "def save_model(model, filename):\n",
        "    filepath = os.path.join(MODEL_DIR, filename)\n",
        "    torch.save(model.state_dict(), filepath)\n",
        "    print(f\"Model saved to '{filepath}'\")\n",
        "\n",
        "# Function to load model parameters\n",
        "def load_model(model, filename):\n",
        "    filepath = os.path.join(MODEL_DIR, filename)\n",
        "    model.load_state_dict(torch.load(filepath))\n",
        "    print(f\"Model loaded from '{filepath}'\")\n",
        "\n",
        "# Training and saving the model\n",
        "num_episodes = 10000  # Number of episodes for training\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    g = Game()\n",
        "    player1 = RandomPlayer()\n",
        "    player2 = DeepRLPlayer()\n",
        "    countWin = 0\n",
        "    countLoss = 0\n",
        "\n",
        "    for _ in tqdm(range(num_episodes), desc=\"Training\"):\n",
        "        winner = g.play(player1, player2)\n",
        "        player2.train()  # Train the agent after each episode\n",
        "        if winner == 1:\n",
        "            countWin += 1\n",
        "        else:\n",
        "            countLoss += 1\n",
        "\n",
        "    trainWinRate = (countWin/num_episodes)*100\n",
        "    trainLossRate = (countLoss/num_episodes)*100\n",
        "    model_filename = f\"deep_rl_player_10.pth, TrainWinRate:{trainWinRate},roundNum: {num_episodes}, lr:, {learning_rate}, epsilon:{epsilon}, structure: 256_512\"  # Filename to save the model\n",
        "    # Save the trained model\n",
        "    save_model(player2.model, model_filename)\n",
        "    print(\"Training completed. Model saved.\")\n",
        "\n",
        "    print(\"TrainWinRate: \", trainWinRate, \"TrainLossRate: \", trainLossRate)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CiWLChLkrQiR",
        "outputId": "fa85aa57-d3d2-4723-f423-c9c2d3fd4619"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 10000/10000 [18:19<00:00,  9.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to 'trained_models/deep_rl_player_10.pth, TrainWinRate:82.53,roundNum: 10000, lr:, 0.001, epsilon:0.01, structure: 256_512'\n",
            "Training completed. Model saved.\n",
            "TrainWinRate:  82.53 TrainLossRate:  17.47\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.  We have tested the trained model in 100 games and the result was 87% win rate."
      ],
      "metadata": {
        "id": "_KMEIDLCGGzv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "g = Game()\n",
        "player1 = RandomPlayer()\n",
        "player2 = DeepRLPlayer()\n",
        "\n",
        "# Load the trained model\n",
        "load_model(player2.model, model_filename)\n",
        "print(\"Model loaded.\")\n",
        "totalGames = 100\n",
        "RgWin = 0\n",
        "RgLoss = 0\n",
        "for _ in tqdm(range(totalGames), desc = \"Matching\"):\n",
        "  winner = g.play(player1, player2)\n",
        "  if winner == 1:\n",
        "    RgWin += 1\n",
        "  elif winner == 0:\n",
        "    RgLoss += 1\n",
        "\n",
        "print(\"\\n WinRate: \", (RgWin/totalGames)*100,\"%\", \"LossRate: \", (RgLoss/totalGames)*100,\"%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2z3mMcN8AMC",
        "outputId": "2e909896-ff57-4715-c7cb-9916b5027bc1"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded from 'trained_models/deep_rl_player_10.pth, TrainWinRate:82.53,roundNum: 10000, lr:, 0.001, epsilon:0.01, structure: 256_512'\n",
            "Model loaded.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Matching: 100%|██████████| 100/100 [00:10<00:00,  9.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " WinRate:  87.0 % LossRate:  13.0 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}